\documentclass[a4paper,10pt]{article}
\usepackage[T1]{fontenc}
\usepackage[u  tf8]{inputenc}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
%\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{natbib}
\usepackage{amsmath}
%\usepackage{float}
%\usepackage{subcaption}
%\usepackage{multirow} 
\usepackage[colorlinks = true,urlcolor=blue]{hyperref}
%\usepackage{bibunits}
%\usepackage{csvsimple}
%\usepackage[superscript,biblabel]{cite}

\newcommand{\jb}[1]{{\color{blue} (#1)} }
\begin{document}

\title{Notes on Dominance and the $Q_X$ Test}

\author{
  Kyelee Ruth Fitts and Jeremy Berg
}

\date{}

\maketitle

So our task is to expess the $Q_X$ statistic in terms of the alleles frequencies, the homozygous effect, and the dominance deviation. In equation 12 of \cite{Berg:2014bs}, we showed that, in the case where all populations are equally distant from one another (with that distance measured in terms of the parameter $F_{ST}$), the statistic could be written in terms of the \textit{average} effect as
\begin{equation} \label{eqn:Qxraw}
  Q_X = \frac{1}{V_A F_{ST}} \sum_{m=1}^M \sum_{\ell=1}^L \sum_{\ell\prime=1}^L \alpha_{\ell} \alpha_{\ell^{\prime}}\left(p_{m\ell} - \overline{p}_\ell \right)\left(p_{m \ell\prime} - \overline{p}_{\ell\prime}\right)
\end{equation}
where $\alpha_\ell$ and $\alpha_{\ell\prime}$ are the average effects for site $\ell$ and $\ell\prime$ respectively, $p_{m\ell}$ is the allele frequency at site $\ell$ in population $m$, and $\overline{p}_\ell$ is the frequency in the ancestral population before it split into the $M$ present day populations (and similar for $\ell\prime$). I've also rewritten in a slightly different way than I did in the \cite{Berg:2014bs}. I've rewritten the sum over $\ell$ and $\ell\prime$ as two distinct sums for the sake of clarity, rather than the shorthand I used in the paper to collapse them into one. And for now I'm assuming that we know the ancestral allele frequency, rather than taking the mean, which is why the sum over $m$ goes from 1 to $M$ rather than $M-1$.

Now, if we assume that the average effects are estimated in population 1, then given the allele frequency in population 1 ($p_{1\ell}$), the difference between homozygotes ($A_\ell$), and the dominance deviation ($D_\ell$) the average effect is given by
\begin{equation}  \label{eqn:avgeff}
  \alpha_\ell = \frac{1}{2} A_\ell + D_\ell\left(1-2p_{1\ell}\right).
\end{equation}

What we want is to reexpress eqn \eqref{eqn:Qxraw} by substituting eqn \eqref{eqn:avgeff} in for the average effects and then simplifying and partitioning in a way that provides insight. Specifically, we'll want to partition into components corresponding to the population in which the GWAS was done (i.e. population 1) and the others, as we expect the bias should come from the fact that $p_{1\ell}$ appears in the expression for the average effect (it'd probably be easiest to first consider a case with just two populations). Nested within that partition among populations, we'll also want to partition into an $F_{ST}$-like or variance term, which should correspond to a sum over terms where $\ell = \ell\prime$, as well as an LD-like or covariance term, which should correspond to a sum over terms where $\ell \neq \ell\prime$. Where possible, you'll want to turn sums into expectations, and expectations into variances or covariances. I supsect you're familiar with these, but just to be sure, you will probably want to make extensive use of the following identities
\begin{align}
  N \mathbb{E}[X] &= \sum_{i=1}^N X_i \\
  Var(X) &= \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\
  Cov(X,Y) &= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\end{align}

and you may find a use for some of the expressions in
\cite{Bohrnstedt:1969cz} regarding the covariance of products of
random variables (or not, I'm not sure). Write your algebra out below
in \LaTeX. You don't need to include every single piece of algebra,
but make sure there's enough that I should be able to follow what
you're doing.

\vspace{10mm}

For the case of two populations:

\begin{equation}
  \sum_{l=1}^L \alpha_l (p_{1l}-\epsilon_l) \sum_{l'=1}^L \alpha_{l'} (p_{1l'} - \epsilon_{l'}) + \sum_{l=1}^L \alpha_l (p_{2l}-\epsilon_l) \sum_{l'=1}^L \alpha_{l'} (p_{2l'} - \epsilon_{l'}) 
  \label{two_pop}
\end{equation}

Taking only the first term:

\begin{equation}
  \sum_{l=l'}^L (\frac{1}{2}A_l + D_l(1-2p_{1l}))^2
  (p_{1l}-\epsilon_l)^2 \sum_{l \neq l'}^L (\frac{1}{2}A_l +
  D_l(1-2p_{1l}))(\frac{1}{2}A_{l'} +
  D_{l'}(1-2p_{1l'}))(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'}) \label{2}
\end{equation}

Another track: forgetting about turning summations into variances and
expectations for a moment, we merely expand the first and second terms
of equation 7 in order to recombine them into summations where both l
and l' start at 1. 

\begin{align}
  \begin{split}
 \text{Term 1: } \sum_{l=l'} (\frac{1}{2}A_l(p_{1l}-\epsilon_l))^2 +
  \sum_{l = l'}(D_l(1-2p_{1l})(p_{1l}-\epsilon_l))^2
  \end{split}
  \\
  \begin{split}
\text{Term 2:} \sum_{l \neq l'}
\frac{1}{4}A_lA_{l'}(p_{1l}-\epsilon_{l})(p_{1l'}-\epsilon_{l'})+D_l(1-2p_{1l})D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_{l})(p_{1l'}-\epsilon_{l'})+\\
\frac{1}{2}A_{l'}D_{l}(1-2p_{1l})(p_{1l}-\epsilon_{l})(p_{1l'}-\epsilon_{l'})\frac{1}{2}A_{l}D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_{l})(p_{1l'}-\epsilon_{l'})
\end{split}
\end{align}

If we multiply Term 1 and Term 2 above, the first term we'll get is:

\begin{equation}
  \begin{split}
  \sum_{l=l'}( \frac{1}{2}A_l(p_{1l}-\epsilon_{l}))^2
  \sum_{l \neq l'}\frac{1}{4}A_lA_{l'}(p_{1l}-\epsilon_{l})(p_{1l'}-\epsilon_{l'})
  = \\
  \sum^L_{l=1}\frac{1}{2}A_l(p_{1l}-\epsilon_{1}) \sum^L_{l'=1}\frac{1}{2}A_{l'}(p_{1l'}-\epsilon_{l'})
\end{split}
\end{equation}

Putting it all together:

\begin{equation}
  \begin{split}
   \sum^L_{l=1}\frac{1}{2}A_l(p_{1l}-\epsilon_{l})
   \sum^L_{l'=1}\frac{1}{2}A_{l'}(p_{1l'}-\epsilon_{l'}) +
   \\
   \sum_{l=l'}(\frac{1}{2}A_l(p_{1l}-\epsilon_l))^2 \sum_{l \neq
     l'}D_l(1-2p_{1l})D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_l)(p_{l1}-\epsilon_{l'})+ 
   \\
     \sum_{l=l'}(\frac{1}{2}A_l(p_{1l}-\epsilon_l))^2\sum_{l \neq
       l'}\frac{1}{2}A_{l}D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'})+
     \\
     \sum_{l=l'}(\frac{1}{2}A_l(p_{1l}-\epsilon_l))^2\sum_{l \neq
       l'}\frac{1}{2}A_{l'}D_l(1-2p_{1l})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'})+
   \\
   \sum_{l=l'}(D_l(1-2p_{1l})(p_{1l}-\epsilon_l))^2 \sum_{l \neq
     l'}\frac{1}{4}A_lA_{l'}(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'}) +
   \\
   \sum_{l=l'}(D_l(1-2p_{1l})(p_{1l}-\epsilon_l))^2 \sum_{l\neq
     l'}
   \frac{1}{2}A_{l'}D_l(1-2p_{1l})(p_{1l}-\epsilon_l)(p_{1l}-\epsilon_{l'})+
   \\
   \sum_{l=l'}(D_l(1-2p_{1l})(p_{1l}-\epsilon_l))^2 \sum_{l\neq
     l'}
   \frac{1}{2}A_{l}D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'})+
   \\
    \sum^L_{l=1}D_l(1-2p_{1l})(p_{1l}-\epsilon_l) \sum^L_{l'=1}D_{l'}(1-2p_{1l'})(p_{1l'}-\epsilon_{l'})
 \end{split} \label{big-expansion}
\end{equation}

   
The equations in 18 agree more with the expressions for turning sums
into expectations, but at the same time take us father from the goal
of partitioning into $ F_{s}$-like and LD-like terms that correspond
to sums where $l = l'$ and $ l \neq l'$, respectively. 

to sums where $l = l'$ and $ l \neq l'$, respectively.

\jb{comment from Jeremy}
So I think you can rearrange equation 10 as
\begin{align}
  \sum^L_{l=1}\frac{1}{2}A_l(p_{1l}-\epsilon_{1}) \sum^L_{l'=1}\frac{1}{2}A_{l'}(p_{1l'}-\epsilon_{l'}) =   \frac{1}{4}\sum^L_{l=1}\sum^L_{l'=1}A_l(p_{1l}-\epsilon_{1})A_{l'}(p_{1l'}-\epsilon_{l'}) \\
                                             = \frac{1}{4}\left(\sum^L_{l=1}(A_l(p_{1l}-\epsilon_{1}))^2 + \sum^L_{l=1}\sum^L_{l\neq l'}(A_l(p_{1l}-\epsilon_{1}))(A_{l'}(p_{1l'}-\epsilon_{1}))\right) \label{fst-ld-partition}\\
                                             = \frac{1}{4}\left(\sum_{l=1}^L Var\left(A_l(p_l - \epsilon_l)\right) + \sum_{l \neq l'}   Cov\left(A_l(p_l - \epsilon_l) , A_{l'}(p_{l'} - \epsilon_{l'})\right)\right) \label{penult} \\
                                             =  \frac{1}{4}\left(\sum_{l=1}^L A_l^2 Var\left(p_l - \epsilon_l\right) + \sum_{l \neq l'} A_l A_{l'} Cov\left(p_l - \epsilon_l , p_{l'} - \epsilon_{l'}\right)\right) \label{ult}
  \end{align}

  This expression lines is essentially the one from my paper (in the case where $D=0$ for all loci, the additive effects ($\alpha$) just reduce to $\frac{A}{2}$. The trick to get from line \eqref{penult} to line \eqref{ult} is that the $A_l$ are constants with respect to the evolutionary process (i.e. genetic drift), and therefore can be pulled outside of the variance and covariance terms.

  Ultimately, in order to work out what the bias in the statistic is with dominance we need to work out its expected value in the presence of dominance. Comparing this to the expected value in the case of no dominance will give us a sense of how strong the bias is. For example, in the case of no dominance, the statistic is just given by line \eqref{ult}. When we take the expectation, the second term in line \eqref{ult} is zero, because alleles that are not tightly linked to one another drift independently (i.e. $\mathbb{E}[Cov(p_l -\epsilon_l,p_{l'}-\epsilon_{l'})] = 0$ for all $l$ and $l'$). This leaves the expectation of the numerator of our statistic as
  \begin{align}
    \frac{1}{4}\mathbb{E}\left[\sum_{l=1}^L A_l^2 Var(p_l - \epsilon_l)\right]
    \end{align}
    and we can use the \href{https://en.wikipedia.org/wiki/Expected_value#Linearity}{linearity of expectation} to push the expectation inside the sum
    \begin{align}
      \frac{1}{4}\sum_{l=1}^L \mathbb{E}\left[A_l^2 Var(p_l - \epsilon_l)\right]
    \end{align}
    which is a familiar quanitity in population/quantitative genetics, such that we can reexpress it in terms of parameters like $F_{ST}$ and $V_A$, but we'll leave that for later.

    My intution says that the decomposition of the test statistic that we're looking for includes terms that look like 
    \begin{align}
      \sum^L_{l=1}\left(\frac{A_l}{2}D_l(1-2p_l)(p_{1l}-\epsilon_{1})^2\right) + \sum^L_{l=1}\sum^L_{l\neq l'}\left(\Bigg(\frac{A_l}{2}(p_{1l}-\epsilon_{1})\Bigg)\Bigg(D_{l'}\left(1-2p_{l'}\right)\left(p_{1l'}-\epsilon_{1}\right)\Bigg)\right) \label{add-times-dom}
    \end{align}
    and
    \begin{align}
      \sum^L_{l=1}\left(\Big(D_l(1-2p_l)(p_{1l}-\epsilon_{1})\Big)^2\right) + \sum^L_{l=1}\sum^L_{l\neq l'}\left(\Bigg(D_{l}\left(1-2p_{l}\right)\left(p_{1l}-\epsilon_{1}\right)\Bigg)\Bigg(D_{l'}\left(1-2p_{l'}\right)\left(p_{1l'}-\epsilon_{1}\right)\Bigg)\right) \label{dom-squared}
    \end{align}
    and in fact I suspect that the ultimate expression we're looking for is basically a sum of line \eqref{fst-ld-partition}, 2 times line \eqref{add-times-dom}, and line \eqref{dom-squared}.

    Let's step back from worrying about variances and covariances for
    a minute (I think I was trying to have you solve too many steps at
    once by suggesting to focus on that interpretation). See if you
    can see your way to verifying my intuition about the above
    expressions. The stuff you have written in lines
    \eqref{big-expansion} seems very close to what we're looking for,
    but 1) I think you may be missing some addition signs in between
    the two different sums on each line (but I'm not certain, as I had
    a little trouble following exactly how you arrived at these
    expressions), and 2) I don't totally follow your indexing, as for
    each case you say that $l$ goes from $l'$ to $L$ but at no point
    is it specified what $l'$ is, so that seems like it can't be quite
    right.



 \vspace{10mm}

 \jb{Continuation of work}

 \vspace{5mm}

 Looking at equation \eqref{big-expansion} it looks like the last
 term with the dominance deviations can be simplified into the
 expression in \eqref{dom-squared} following the same steps that led
 to \eqref{penult}. The problem is the middle terms in
 \eqref{big-expansion}. My first thought is to factor out the
 summations with $l=l'$ because those are the same for the first three
 and the second three terms. 

 \begin{equation}
   \begin{split}   
   \sum_{l=l'}(\frac{1}{2}A_l(p_{1l}-\epsilon_l))^2 (\sum_{l
     \neq
     l'}D_l(1-2p_{1l})D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_l)(p_{l1}-\epsilon_{l'})+
   \\
   \sum_{l \neq l'}\frac{1}{2}A_{l}D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'})+
   \\
   \sum_{l \neq l'}\frac{1}{2}A_{l'}D_l(1-2p_{1l})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'}))
   \\
   + \sum_{l=l'}(D_l(1-2p_{1l})(p_{1l}-\epsilon_l))^2 (\sum_{l
     \neq l'}\frac{1}{4}A_lA_{l'}(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'})+
   \\
   \sum_{l \neq l'}\frac{1}{2}A_{l'}D_l(1-2p_{1l})(p_{1l}-\epsilon_l)(p_{1l}-\epsilon_{l'})+
   \\
   \sum_{l \neq l'} \frac{1}{2}A_{l}D_{l'}(1-2p_{1l'})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'}))
   \end{split}
\end{equation}\label{factored} 


Okay this approach is going nowhere. I'm going to go back to equation
\eqref{two_pop} and try to expand it out using the same technique as
Jeremy used to get to \eqref{penult}, and see where I can go from
there.


\begin{equation}
  \begin{split}
    \sum^L_{l=1} \sum^L_{l'=1}(\frac{1}{2}A_l +
    D_l(1-2p_{1l}))(p_{1l}-\epsilon_l)*(\frac{1}{2}A_{l'} +
    D_{l'}(1-2p_{1l'}))(p_{1l'}-\epsilon_{l'}) \\
    =\sum^L_{l=1}(\frac{1}{2}A_l+D_l(1-2p_{1l}))^2(p_{1l}-\epsilon_l)^2+ \\
    \sum^L_{l=1}\sum^L_{l \neq
    l'}(\frac{1}{2}A_l(p_{1l}-\epsilon_l)+D_l(1-2p_{1l})(p_{1l}-\epsilon_l))(\frac{1}{2}A_{l'}(p_{1l'}-\epsilon_{l'})+D_{l'}(1-2p_{1l'})(p_{1l'}-\epsilon_{l'})) \label{second_expansion_penult}
  \end{split}
\end{equation} 

With some distributing and rearranging, this equals: 

\begin{equation}
  \begin{split}
  \sum^L_{l=1}\frac{1}{4}A_l^2(p_{1l}-\epsilon_l)^2+\sum^L_{l=1}\sum^L_{
    \neq
    l'}(\frac{1}{4}A_l(p_{1l}-\epsilon_{l})A_{l'}(p_{1l'}-\epsilon_{l'}))
  \\
  +\sum^L_{l=1}A_lD_l(1-2p_{1l})(p_{1l}-\epsilon_l)^2 +
  \sum^L_{l=1}\sum^L_{l \neq
    l'}\frac{1}{2}A_lD_{l'}(1-2_{p1l'})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'}) \\
  +D_lA_{l'}(1-2_{p1l})(p_{1l}-\epsilon_l)(p_{1l'}-\epsilon_{l'}) \\
   \sum^L_{l=1}(D_l(1-2p_l)(p_{1l}-\epsilon_{1}))^2)
   + \sum^L_{l=1}\sum^L_{l\neq
     l'}D_{l}(1-2p_{l})(p_{1l}-\epsilon_{1})D_{l'}(1-2p_{l'})(p_{1l'}-\epsilon_{1}) \label{second_exp_ult}
  \end{split}
\end{equation}

Helpful resource:
\href{http://www.statpower.net/Content/310/Summation%20Algebra.pdf}{link}.  

Equation \eqref{second_exp_ult} is almost exactly what Jeremy's
intuition specifies, with the exception of the second term, which
has instead a sum of the combination of $l$ and $l'$ subscripts for
the average effect and dominant terms. We now have expressions that
look very much like \eqref{add-times-dom} and \eqref{dom-squared}. I
can turn these terms into variances and covariances in an analogous
manner.

\begin{equation}
  \begin{split}
\frac{1}{4}\sum^L_{l=1}Var(A_l(p_{1l}-\epsilon_l)) + \sum_{l \neq
  l'}Cov(A_l(p_{1l}-\epsilon_{l'})), A_{l'}(p_{l'}-\epsilon_{l'}))
  \end{split}
\end{equation}

    
\bibliographystyle{genetics}
\bibliography{Jeremy_library}

\end{document}

